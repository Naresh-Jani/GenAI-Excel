{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj3fFYgoXHkt",
        "outputId": "e97c8415-b51a-4b3f-e86a-217eb00742ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "page_content='LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for chains, many integrations with other tools, and end-to-end chains for common applications.\n",
            "\n",
            "LangChain allows AI developers to develop applications based on the combined Large Language Models (such as GPT-4) with external sources of computation and data. This framework comes with a package for both Python and JavaScript.\n",
            "\n",
            "Why is LangChain Important?\n",
            "LangChain helps manage complex workflows, making it easier to integrate LLMs into various applications like chatbots and document analysis. Key benefits include:'\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "with open(\"/content/MyData.txt\") as f:\n",
        "    mydata = f.read()\n",
        "\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "chunks = text_splitter.create_documents([mydata])\n",
        "print(chunks[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "texts = text_splitter.create_documents([mydata])\n",
        "print(\"Chunk 1:\",texts[0])\n",
        "print(\"Chunk 2:\",texts[1])\n",
        "print(\"Chunk 3:\",texts[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjhLtLh4Xm1K",
        "outputId": "b116aa72-b6bf-4799-c63f-b6809a46f81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 1: page_content='LangChain is an open-source framework designed to simplify the creation of applications using large'\n",
            "Chunk 2: page_content='using large language models (LLMs). It provides a standard interface for chains, many integrations'\n",
            "Chunk 3: page_content='many integrations with other tools, and end-to-end chains for common applications.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs=model_kwargs\n",
        ")\n",
        "\n",
        "text_splitter = SemanticChunker(embeddings)\n",
        "\n",
        "docs = text_splitter.create_documents([mydata])\n",
        "print(docs[0].page_content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8_KXPVNx7LG",
        "outputId": "b42770df-1f45-48dd-a931-ac94a60a731b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is an open-source framework designed to simplify the creation of applications using large language models (LLMs). It provides a standard interface for chains, many integrations with other tools, and end-to-end chains for common applications. LangChain allows AI developers to develop applications based on the combined Large Language Models (such as GPT-4) with external sources of computation and data. This framework comes with a package for both Python and JavaScript. Why is LangChain Important? LangChain helps manage complex workflows, making it easier to integrate LLMs into various applications like chatbots and document analysis. Key benefits include:\n",
            "\n",
            "Modular Workflow: Simplifies chaining LLMs together for reusable and efficient workflows. Prompt Management: Offers tools for effective prompt engineering and memory handling. Ease of Integration: Streamlines the process of building LLM-powered applications. Key Components of LangChain\n",
            "1. Chains\n",
            "Chains define sequences of actions, where each step can involve querying an LLM, manipulating data, or interacting with external tools. There are two types:\n",
            "\n",
            "Simple Chains: A single LLM invocation. Multi-step Chains: Multiple LLMs or actions combined, where each step can take the output from the previous step. 2. Prompt Management\n",
            "LangChain facilitates managing and customizing prompts passed to the LLM. Developers can use PromptTemplates to define how inputs and outputs are formatted before being passed to the model. It also simplifies tasks like handling dynamic variables and prompt engineering, making it easier to control the LLM’s behavior.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[2].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBAiDeW-0WC8",
        "outputId": "f0ae93b1-f99d-463e-c4b4-bc27d0f1a7c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6. Memory Management\n",
            "LangChain supports memory management, allowing the LLM to “remember” context from previous interactions. This is especially useful for creating conversational agents that need context across multiple inputs. The memory allows the model to handle sequential conversations, keeping track of prior exchanges to ensure the system responds appropriately. 2011 Cricket world cup history:\n",
            "The 2011 ICC Cricket World Cup was the tenth Cricket World Cup. It was played in India, Sri Lanka and Bangladesh, while the latter hosted World Cup matches for the first time. India won the tournament, defeating Sri Lanka by six wickets in the final at Wankhede Stadium in Mumbai, thus becoming the first country to win the Cricket World Cup final on home soil.[1][2] India's Yuvraj Singh was declared as the player of the tournament.[3] This was the first time in World Cup history that two Asian teams had appeared in the final. It was also the first time since the 1992 World Cup that the final did not feature Australia. Fourteen national cricket teams took part in this tournament, including 10 full members and four associate members of the International Cricket Council (ICC).[4] The opening ceremony was held on 17 February 2011 at Bangabandhu National Stadium, Dhaka,[5] and the tournament was played between 19 February and 2 April. The first match was played between India and Bangladesh at the Sher-e-Bangla National Stadium in Mirpur, Dhaka.[6]\n",
            "\n",
            "Pakistan was also scheduled to be a co-host, but after the 2009's terrorist attack on the Sri Lanka national cricket team in Lahore, the ICC cancelled that,[7] and the headquarters of the organising committee, originally in Lahore, was transferred to Mumbai.[8] Pakistan was to have held 14 matches, including one semi-final.[9] Eight of the games (including the semi-final) were awarded to India, four to Sri Lanka, and two to Bangladesh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import PythonCodeTextSplitter\n",
        "\n",
        "python_text = \"\"\"\n",
        "class Product:\n",
        "  def __init__(self, name, type):\n",
        "    self.name = name\n",
        "    self.type = type\n",
        "\n",
        "p1 = Product(\"Television\", \"Electronics\")\n",
        "\n",
        "for i in range(10):\n",
        "    print (i)\n",
        "\"\"\"\n",
        "\n",
        "python_splitter = PythonCodeTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "print(python_splitter.create_documents([python_text]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1phstgc1qPc",
        "outputId": "ce82b61b-b8dd-4d4a-e9cf-adc815ad4add"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(metadata={}, page_content='class Product:\\n  def __init__(self, name, type):\\n    self.name = name\\n    self.type = type'), Document(metadata={}, page_content='p1 = Product(\"Television\", \"Electronics\")\\n\\nfor i in range(10):\\n    print (i)')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import SentenceTransformersTokenTextSplitter\n",
        "\n",
        "splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=0)\n",
        "text = \"LangChain supports memory management\"\n",
        "\n",
        "count_start_and_stop_tokens = 2\n",
        "text_token_count = splitter.count_tokens(text=text) - count_start_and_stop_tokens\n",
        "print(text_token_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFNMcn9J3Qth",
        "outputId": "2cc5703c-9cde-4a6a-9140-3856f1dd70db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import TokenTextSplitter\n",
        "import tiktoken\n",
        "text = \"LangChain supports memory management, allowing the LLM to “remember” context from previous interactions.\"\n",
        "text_splitter = TokenTextSplitter(chunk_size=10, chunk_overlap=0)\n",
        "\n",
        "texts = text_splitter.split_text(text)\n",
        "print(texts[0])\n",
        "print(texts[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVpp37pe3rNA",
        "outputId": "bd6315f2-7888-473f-84a8-8e64d317649b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain supports memory management, allowing the LL\n",
            "M to “remember” context from previous\n"
          ]
        }
      ]
    }
  ]
}